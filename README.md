# Gestura: AI Sign Language Interface

A futuristic, real-time sign language detection interface built with **Next.js**, **MediaPipe**, and **Framer Motion**.

![Next JS](https://img.shields.io/badge/Next-black?style=for-the-badge&logo=next.js&logoColor=white)
![React](https://img.shields.io/badge/React-20232A?style=for-the-badge&logo=react&logoColor=61DAFB)
![TailwindCSS](https://img.shields.io/badge/Tailwind_CSS-38B2AC?style=for-the-badge&logo=tailwind-css&logoColor=white)
![MediaPipe](https://img.shields.io/badge/MediaPipe-0099CC?style=for-the-badge&logo=google&logoColor=white)
![Framer Motion](https://img.shields.io/badge/Framer_Motion-0055FF?style=for-the-badge&logo=framer&logoColor=white)
[![Live Demo](https://img.shields.io/badge/ğŸš€_Live_Demo-Click_Here-brightgreen?style=for-the-badge)](https://Goddex-123.github.io/Gestura_ML/)

> **ğŸ¯ Try it now!** Click the **Live Demo** badge above to experience the AI interface instantly!

## ğŸš€ Features

| Feature                    | Description                                               |
| -------------------------- | --------------------------------------------------------- |
| âœ‹ **Real-Time Tracking**  | High-precision hand gesture detection using MediaPipe     |
| âš¡ **Zero Latency**        | Optimized WebGL rendering for smooth 60fps performance    |
| ğŸ¨ **Cyberpunk Aesthetic** | Glassmorphism ui, neon glows, and fluid motion animations |
| ğŸ§  **AI-Powered**          | Neural network integration for instant sign translation   |
| ğŸ“± **Responsive Design**   | Fully optimized for desktop, tablet, and mobile devices   |

## ğŸ“¸ Screenshots

_Coming soon..._

## ğŸ› ï¸ Tech Stack

- **Framework:** Next.js 14 (App Router)
- **Computer Vision:** Google MediaPipe Hands
- **Styling:** Tailwind CSS
- **Animations:** Framer Motion
- **State Management:** React Hooks
- **Language:** TypeScript

## ğŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/Goddex-123/Gestura_ML.git
cd Gestura_ML

# Install dependencies
npm install

# Run the development server
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) with your browser to see the result.

## ğŸ¯ Usage

1.  **Launch the app** from the Live Demo or locally.
2.  **Allow Camera Access** to enable real-time tracking.
3.  **Perform Gestures** in front of the camera.
4.  **View Translations** instantly on the screen as you sign.

## ğŸ“ Project Structure

```
Gestura_ML/
â”œâ”€â”€ .github/          # GitHub Actions workflows
â”œâ”€â”€ public/           # Static assets (images, models)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/          # Next.js App Router pages
â”‚   â”œâ”€â”€ components/   # React UI components
â”‚   â””â”€â”€ utils/        # Helper functions
â”œâ”€â”€ next.config.ts    # Next.js configuration
â”œâ”€â”€ package.json      # Project dependencies
â””â”€â”€ README.md
```

## ğŸ¤ Contributing

Contributions are welcome! Feel free to:

- Report bugs
- Suggest features
- Submit pull requests

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¨â€ğŸ’» Author

**Soham** - AI & Web Developer

---

â­ Star this repo if you found it useful!

---

[ğŸ“œ Contribution Log](file:///d:/Soham_1/gestura-ml/CONTRIBUTIONS.md)
